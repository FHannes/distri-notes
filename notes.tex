% Samenvatting van het vak Gedistribueerde Systemen
\documentclass[a4paper]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage[pdftex]{graphicx}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{float}
\usepackage{framed}
\usepackage{varwidth}
\usepackage{titlesec}
\usepackage{varwidth}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{qrcode}

\lstset{
    language=Java,
    basicstyle=\small\ttfamily,
    keywordstyle=\bf\ttfamily\color[rgb]{0,.3,.7},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle={\color[rgb]{0.75,0.49,0.07}},
    numberstyle=\small\ttfamily\color[rgb]{0.7,0.7,0.7},
    breaklines=true,
    frame=leftline
}
\lstMakeShortInline[columns=fixed]|

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}

\begin{document}

\begin{titlepage}
    \newpage
    \thispagestyle{empty}
    \frenchspacing
    \hspace{-0.2cm}
    \includegraphics[height=3.4cm]{assets/sedes}
    \hspace{0.2cm}
    \rule{0.5pt}{3.4cm}
    \hspace{0.2cm}
    \begin{minipage}[b]{8cm}
        \large{Katholieke\newline Universiteit\newline Leuven}\smallskip\newline
        \large{}\smallskip\newline
        \textbf{Department of\newline Computer Science}\smallskip
    \end{minipage}
    \hspace{\stretch{1}}
    \vspace*{3.2cm}\vfill
    \begin{center}
        \begin{minipage}[t]{\textwidth}
            \begin{center}
                \large{\rm{\textbf{\uppercase{Notes}}}}\\
                \large{\rm{Distributed Systems [H0N08a]}}\\
                \large{\rm{Gedistribueerde Systemen [H04I4a]}}
            \end{center}
        \end{minipage}
    \end{center}
    \vfill\vspace*{5cm}
    \begin{minipage}[t]{0.2\textwidth}
        \begin{center}
            @GitHub\\
            \qrcode{https://github.com/FHannes/distri-notes}\\
            {\tt \#\input{rev}}
        \end{center}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.3\textwidth}
        \vfill\noindent \\[5mm]
        {\rm \textbf{Fr\'ed\'eric Hannes}}\\[2mm]
        {\rm Academic year 2015--2016}
    \end{minipage}
\end{titlepage}

\tableofcontents

\newpage

\section{Introduction}

\subsection{Definitions}

A \textbf{distributed system} is a system which consists out of hardware and/or software components, a network which provides a connection between those components and communication to provide coordination of the components by passing messages.

\textbf{Concurrency} can be a major issue for distributed systems as it is very difficult to synchronize events across the various components. One of the reasons for this is the lack of a \textbf{global clock}. As such, time might be different for different components of the system. Another hurdle are \textbf{independent failures} of these system components. In a distributed environment, a system will never completely fail, but each component of the system can fail independently. The system van handle these failures in various ways. Full availability of a distributed system can never be guaranteed.

A \textbf{distributed algorithm} is a collection of cooperating algorithms which communicate through message passing. An example algorithm is mutual exclusion, which prevents different processes from using the same resource simultaneously.

\subsection{Examples}

\subsubsection{Example 1: Internet}

The internet is a vast collection of computer networks. These are (small) local intranets which are connected by backbone machines. ISPs provide connectivity between these networks, as well various services. Services on the internet include the world-wide web, consisting out of webpages, but also email for messaging and file transfer to move files through the networks.

Each local network which connects to the internet has its own security policies and administration. They are connected to the internet through routers. Firewalls are used as a security system by filtering messages passing between networks and machines at the routers.

The internet and its services were constructed to increase the sharing of resources. These resources include files, physical machines such as printers and databases.

\subsubsection{Example 2: Mobile \& ubiquitous computing}

The miniaturization of electrical components and advances in wireless networking have made it possible to create laptops, handheld devices such as PDAs and mobile phones, wearable computers such as smart watches and smart cards and also embedded devices like washing machines, cars and hi-fi systems.

Most of these systems can be referred to as \textbf{mobile computing} devices. Mobile computing is ubiquitous computing, as the devices can be used anywhere and can communicate with each other. The mobile devices move in and out of intranets, but require transparent access to the home intranet, meaning that the user should be unaware that the device is not on its local intranet and should be able to access the resources on that intranet as if it were local. This form of access to local resource at a remote site is \textbf{location-aware computing}.

Some of the challenges with mobile computing include automatically reconfiguring the host intranet and the mobile device when it enters or leaves this intranet. It also has to cope with limited connectivity. There are also various privacy and security concerns to take into account when a mobile device uses a foreign intranet.

\subsubsection{Example 3: Commercial applications}

There are various commercial applications of distributed systems, each with its own challenges. An example of e-commerce, more specifically home banking, has the challenge of correctly completing transactions. If a user transfers money to another person, they want this to happen just once and correctly. This means that the operation has to be \textbf{idempotent}. If it is performed twice, it should have the same result as if it were only performed once. In the case of the bank, this means that a specific amount shouldn't be transferred by the operation, but rather that the operation should increase the amount of the target account before the operation with a certain value, to achieve a specific new balance. That way, even if the operation is executed twice, the end-result will still be the same.

For health care, global access to patient information is a goal which is being worked towards. The idea behind this is that patient information can be shared between hospitals in the same country or even across borders if the patient allows it, as there are privacy concerns involved. Mechanisms must also be in place to give medical professionals the ability to overrule these mechanisms if a patient is incapable of making the decision that this data can be shared.

Audio and video calling is an example for telecommunication, where systems must handle real-time traffic. The data must reach its destination fast enough to allow for smooth, uninterrupted communication.

In financial trading, complex event processing engines which are driven by trading strategies are connected at physical locations close to the stock exchanges, to minimize delays, allowing for faster and more accurate trades. In these markets, it is important that a trade is executed within milliseconds as to ensure that the price at which it is done so, doesn't differ greatly from what was observed or intended when the transaction was initiated.

\subsection{Trends}

\subsubsection{Pervasive networking}

\textbf{Pervasive networking} is the idea of the internet of things, where there is a trend that more and more products are able to communicate in some way with a network. This now includes even tiny sensors which operate at nano scales.

\subsubsection{Increasing mobility}

There's a trend of ever increasing mobility, with a rapidly growing number of mobile devices communicating with networks. This results in increasingly ubiquitous computing. \textit{This topic is handled in the course Capital Selecta Distributed Systems: Applications of Sensor Networks}.

\subsubsection{Multimedia systems}

Multimedia systems are increasingly acting as a distributed system, where the devices are interconnected and are able to function as one large unit. \textit{This topic is handled in specialized courses by the HCI group}.

\subsubsection{Utility computing}

\textbf{Utility computing} refers to a popular trend where computing is handled as a utility service. A person or company generally does not want to spend more on anything, including computing, than it needs to. For a utility, you are only charged for the amount of the service or good which is consumed. Utility computing tries to implement this concept for computing resources. Services which are generally referred to with the buzz word "cloud computing", provided by companies such as Amazon, aim to do this. They can provide computing power for which their customer will only be charged for the amount of computing power/time they consumed. \textit{This topic is handled in this course, but is also extensively reviewed in the source Capital Selecta Distributed Systems: Software-as-a-service}.

\subsection{Types of concurrency}

\subsubsection{Interleaved computation}

\textbf{Interleaved computation} implements concurrency on a single processor. This is not true concurrency, as it can not simultaneously perform two tasks, but it attempts to switch between tasks rapidly, as to create the illusion of performing concurrent tasks.

With this method, a job is performed as the execution of a single program. This is done with cooperating subtasks or threads, using the interleaved execution method. The threads are able to communicate with each other using shared memory. There is a single clock, which means that events can be ordered in a schedule for execution.

\subsubsection{Parallel computing}

\textbf{Parallel computing} implements true concurrency on multiprocessor systems. It executes a job as a single program, consisting out of subtasks or threads. This is true concurrency as the threads can be scheduled on the individual processors and executed simultaneously. Threads will still communicate through shared memory as it all occurs on a single system. This also means that there is a single clock, allowing events to be ordered in a schedule.

\subparagraph{Example} Single Instruction, Multiple Data (SIMD) is an example of parallel programming, where a single program is executed and handles different (partial) datasets concurrently.

\subsubsection{Distributed computing}

A job is the execution of many procedures. It consists out of many cooperating tasks. A single process can have subtasks or treads which use real concurrency. Unlike with interleaved or parallel computing, a distributed system can not make use of a shared memory environment to provide communication between threads. Instead processes communicate using message passing over the network which connects the nodes of the distributed system. Each system has its own clock, this means that there can only be a partial ordering for events in the system, as time is no longer a unique value in the system.

\subsubsection{Parallel vs. Distributed}

There are several key differences between parallel and distributed computing:
\begin{itemize}
\item \textbf{Hardware}: Parallel hardware differs greatly from distributed hardware in the way that parallel hardware generally uses identical processors and a regular interconnection structure. Distributed hardware in contrast can consist of various different types of processors and the nodes in a distributed system are interconnected using network. This adds the risk of partial (independent) failure, which can lead to unpredictable results if not handled properly.
\item \textbf{Granularity}: Parallel computing has a small granularity of tasks. This means that tasks are generally hard to distinguish from each other. On a distributed system, there has to be a clear division of labor, which leads to a large granularity of tasks.
\item \textbf{Frequency of communication}: Communication happens frequently for parallel computing, where as it does not for distributed computing. The reason for this is that communication channels in a parallel environment are very fast, which keeps any overhead invoked by communicating, very small. In a distributed environment, nodes of the system are usually connected over a network. These networks, though ever increasing in transfer speeds, are not able to match the performance of the on-board or even on-chip communication channels which are used in a parallel environment. As a result, communication in a distributed environment is kept to a minimum.
\item \textbf{Homogeneity}: In a parallel computing environment, tasks often perform the same function. In a distributed environment, usually tasks will perform different functions. The reasoning behind this is that similar tasks generally require synchronization and communication, which is more difficult in a distributed environment. We say that there is heterogeneity between tasks in a distributed environment.
\item \textbf{Synchronization}: As a parallel system has a global clock, synchronization can easily be achieved by leveraging the fact that time is always the same throughout the system. In a distributed environment, special routines and communication are required to synchronize events.
\item \textbf{Security}: We consider a distributed system to be insecure, as the system has to communicate over a network, which is very vulnerable to outside intrusions, compared to a parallel system, where execution of a program is performed on a single machine.
\item \textbf{Failures}: When a parallel system fails, it fails completely, this is referred to as \textbf{full failure}.  A distributed system will usually not fail entirely, which we refer to as \textbf{independent failure}. These failures can have different causes, but usually either a node in the distributed system will fail, or an entire network segment will become unavailable.
\end{itemize}

\subsection{Challenges}

\subsubsection{Heterogeneity}

Heterogeneity is present at many levels when working with distributed computing. A distributed system can use different types of networks such as Ethernet or token ring networks. It can also be implemented using different kinds of computer hardware and operating systems. A distributed application might rely on applications which have been written in different programming languages and which have been implemented by different developers. To overcome these hurdles, \textbf{middleware} is used. This is software which is placed in between the platform and the application, which ensures the application can run in an environment with standardized components.

Examples of middleware include the Java RMI (Remote Method Invocation) API, which is used to call methods on remote systems. CORBA (Common Object Request Broker Architecture) is a platform which also allows applications to work with objects on remote systems. Both middleware solutions implement a uniform high level API for remote procedures and methods, remote invocation, remote event notification and distributed transactions.

\subsubsection{Openness}

Open systems enable adding system extensions without disruption or duplication of existing services. This means that when systems are open, they can be called upon by other systems, rather than those systems having to implement the same functionality. This can be achieved by using a uniform communication mechanism to enable distributed programming and by publishing interfaces or using standard interfaces to access shared resources. As a result, these open distributed systems are able to use heterogeneous hardware.

\subsubsection{Security}

A distributed system may get attacked in various ways. It may be attacked to breach confidentiality and privacy, by obtaining data from the system. The integrity of messages passed within the system may be compromised. User authentication may be spoofed by simulating a false identity. An attacker may also gain unauthorized access to resources of the system, such as files or devices like printers. A denial of service attack might cripple availability of the system by overwhelming it with requests.

\subsubsection{Scalability}

Scalability is a major challenge as it is very difficult to scale up a system, whilst retaining the same software. It is important to control the cost of physical resources and the loss of performance in a distributed system. One must also make sure to prevent software resources from running out and to avoid performance bottlenecks.

This can be achieved by replicating and partitioning data, so it can be processed in a distributed environment; as well as by caching of data and using of multiple servers.

\subsubsection{Failure handling}

Partial failures are difficult to handle, but several techniques are available:
\begin{itemize}
\item \textbf{Detecting failures}: Detecting failures is an important part of failure handling in general. Mechanisms such as checksums can be used to achieve this.
\item \textbf{Masking failures}: A system can mask a failure by attempting to continue operating after a failure has occurred. If a message has been sent to a node in a distributed system which has failed, this message may be retransmitted to be handled by a different node instead, if no reply was received.
\item \textbf{Tolerating failures}: A failure may be tolerated, which means that it is deemed acceptable for the failure to occur, if it does not occur too frequently. This may not be possible for some critical system, but can often be seen on websites, when it states that a service is not currently available.
\item \textbf{Recovery from failures}: A system may recover from a failure. An example of this would be saving the state of execution, which is then restored later on to resume the execution.
\item \textbf{Redundancy}: Redundancy can be applied to handle failures as well. This si achieved by replicating services. If a service fails, one of the redundant replications can take over, preventing the system from failing.
\end{itemize}

\subsubsection{Concurrency}

Different clients will often try to access a shared resource simultaneously. This is a problem which can be solved in either of two ways. The access can be restricted to a single client, but this is very inefficient. It is also possible to allow concurrent executions, but this is often difficult to implement. Synchronization tools are needed to achieve this. There are many known techniques, such as semaphores which can be implemented.

\subsubsection{Transparency}

A system is transparent for a feature if that feature is unobservable for the user. This means that the user is unaware of the full extent of the implementation of that feature. In Java RMI for example, an object can be remote, but the software will be unaware of this fact and treat the object as ti would a local object. Another example to consider is a mobile application, such as a banking app. To the user, any action taken in the app appears as if it were done locally, where as in reality, a very large and complex (distributed) system is responsible for processing these actions remotely.

Transparency is possible for many different aspects:
\begin{itemize}
\item \textbf{Access}: Access to local and remote resources is identical. The client is not made aware of the difference.
\item \textbf{Location}: Resources can be accessed without knowledge of their physical or network location. Consider a file service such as Dropbox, where a user can access their files, never knowing where in the world (on what server) exactly they are stored.
\item \textbf{Concurrency}
\item \textbf{Replication}
\item \textbf{Failure}
\item \textbf{Mobility}: Allows movement of resources, without functionality being affected.
\item \textbf{Performance}
\item \textbf{Scaling}
\end{itemize}

\newpage

\section{Direct Communication}

Direct communication means that the sender knows the receiver and the receiver has to be online in the same time window.

\subsection{Distribution services}

A \textbf{distribution service} in middleware is part of the larger middleware setting. It shields developers of a distributed application from the environment by providing abstraction layers on top of the OS. This creates a clean interface which hides aspects such as low-level APIs or system specific data structures.

\subsubsection{Data representation}

A problem when working with a heterogenous environment is that different underlying systems may represent program data in different ways. This makes it unclear how this data should be mapped onto a message, to ensure that other systems are able to interpret it correctly. The problem can be resolved by converting the data, which can be done using various approaches.

\begin{itemize}
\item A specific format for transmission can be agreed upon. Data can be converted from the original format on the source system to the intermediary transmission format and converted to the format of the target system upon receival. All machines in the system which communicate must of course be aware of these format specifications. Examples of this are \textit{Sun XDR} and \textit{COBRA CDR}.
\item A description of the data can be transmitted alongside the actual data. This metadata allows a receiver to comprehend the format, even if it was not aware of it previously. Some examples include \textit{ASN + BER} and \textit{Java serialized form}, the latter of which is a data format which Java objects can be serialized into.
\item Another approach is converting the data to an ASCII text format such as XML or JSON.
\end{itemize}

\textbf{XML} is a markup language defined by the WWW consortium. It tags data items with markup strings, which contain metadata. The naming of these tags is chosen by the user, but generally gives some indication regarding the meaning of the value it accompanies.

Converting a collection of data items into a form which is suitable for transmission (or storage) is referred to as \textbf{marshalling}. The disassembly of a message to recreate the collection of data items is called \textbf{unmarshalling}. These operations are usually based off of a specification which determine show they should be performed for a given data type. In Java, these operations are referred to as \textbf{serialization} and \textbf{deserialization} respectively.

\subsubsection{Message passing}

To implement the basic functionality of message passing, 2 methods are required. One to \textbf{send} a given message to a receiver at a specific destination (such as a port), as well as a method to \textbf{receive} a message on a port.

\subsubsubsection{Synchronous vs. asynchronous}

There's a different between synchronous and asynchronous communication.

\begin{itemize}
\item \textbf{Synchronous} or blocking communication waits during the communication. When the |send| method is called, the sender will wait until the receiver has received the message. When the |receive| method is called, the receiver will wait until a message is received. It is referred to as blocking communication because the thread which calls those methods will be blocked, not able to execute other instructions, until the method returns.
\item \textbf{Asynchronous} or non-blocking communication does not wait during communication. When the |send| method is called, the sender will not wait for a notification that the message has been received. When the |receive| method is called, the receiver will either announce its willingness to accept a message or check if a message has arrived. These messages will generally be buffered until the receiver checks if a message has been received.
\end{itemize}

Note that it is not imperative for a system to use synchronous or asynchronous communication for both |send| and |receive| methods, it can very well use a combination. E.g.: The Mach programming language uses an asynchronous |send| method, but a synchronous |receive| method.

Some languages may also be capable of using both synchronous and asynchronous communication, if libraries are available to support them.

\subsubsubsection{Message destination}

A \textbf{message destination} is a communication identifier, which can be used to determine where to send messages. These identifiers are by preference not dependent on a location. There are process, port and mailbox identifiers.

\begin{itemize}
\item A \textbf{process} has a single entry point per process for all messages, the process must then handle the message internally, depending on the purpose of the message.
\item A \textbf{port} is listened on by a single receiver, but can be communicated to by many senders. It may have a message queue, which is often the case for asynchronous communication. Here the message is buffered until it is requested by the application, which is not aware of when exactly the message arrives. A process can also have many ports, this also makes it possible to separate some of the message flow before processing inside of the application.
\item A \textbf{mailbox} can have many receivers. A message is sent to a mailbox and depending on the message's destination parameters, can be distributed to various receivers. Mailboxes generally also queue messages until they are retrieved.
\end{itemize}

\subsubsubsection{Reliability}

When transporting messages, various failures can occur. Some include corruption, duplication, omission (loss) or loss of ordering. All of these are \textbf{communication failures}. Another possibility is that the receiver fails and is not there to receive the messages.

Communication is considered to be reliable when messages are delivered uncorrupted, in order, without duplicates. A pitfall here is that a reasonable number of packets may be dropped or lost, which has to be handled to ensure reliability. It is therefore often not possible to guarantee perfectly reliable communication.

There are three important aspects to implementing reliable communication.

\begin{itemize}
\item Avoiding corruption by including some sort of checksum in the message.
\item Avoiding ordering mistakes as well as duplicates by including a sequential message number in each message.
\item Avoiding omission (loss of messages) by having the receiver reply with an acknowledgement when it receives a message and having the sender retransmit if it times out on waiting for that acknowledgement.
\end{itemize}

It is important to note that it is hard or sometimes impossible to distinguish between process failure and communication failure when the sender received no acknowledgement.

\subsubsubsection{Case study: UDP/TCP}

When communicating using the UDP (\textbf{User Datagram Protocol}) or TCP (\textbf{Transmission Control Protocol}) protocols, sockets are used to communicate. A socket is a virtual construct which is bound to a port (and IP address). That port can then be used by the socket to connect with other machines and communicate with them. The destination port is usually agreed upon beforehand, as a client has to know where to send messages before it does. The port a socket binds to on the client itself is usually chosen at random. When the connection is initiated, the server will be made aware of the port that was selected by the client to establish communications.

\subparagraph*{UDP} The UDP protocol sends messages with a restricted packet size. Messages with a larger size are truncated, which means they are cut off and sent in parts. Usually synchronization happens with a non-block send and blocking |receive| method. Timeouts on the receive operation can be set by the user. These timeouts determine how long the |receive| method will wait for an incoming message. An UDP socket can receive messages from any remove IP and port. Unlike TCP, UDP does not implement any mechanisms to ensure reliability. It should therefore not be used for services where loss of data during communication is unacceptable. Packets could be lost, received out of order or duplicated and it is impossible to verify message corruption.

\subparagraph*{TCP} Unlike the UDP protocol, the TCP protocol uses stream communication as opposed to message passing. It creates a communication channel through sockets upon connecting, which can be written to or read from. The channel can be closed when communication stops. Because TCP uses a stream, there's no boundary between the data of individual messages; the TCP protocol just transmits raw bits/bytes. It is read as a sequence of bytes. It is up to the software to implement a messaging system. There's a buffer on the sender and receiver side, which buffers data that is queued to be transmitted and data which has been received. These buffers are accessed when the sender writes data or when the receiver reads it.

Usually synchronization for TCP occurs by using a non-blocking |send| method and a blocking |receive| method, though this is not always the case and there is an exception for flow control, when the sending or receiving buffers are full. Setting up a connection is done using a handshake protocol. The client sends a request to establish communications to the server port, upon which the server can accept or reject this request. When accepting a request, the server typically creates a new thread to handle communication with the specific client, but some servers also use a working system and queue clients until a worker process is available to handle them.

TCP offers a great advantage over UDP for many applications in that it offers a reliable connection, with the exception of connections which are broken. It ensures that packets arrive without corruption in the correct order.

In contrast to UDP, TCP does have some overhead. The buffering system it employs causes overhead, as well as the handshake protocol used to establish connections. The latter is particularly noticeable if you are only connecting to send a single message, where the handshake protocol adds another two messages to the communications. Another source of overhead is found in assurance of reliability, where TCP ensures that the connection is stable and all data reaches its destination correctly, adding things such as checksums, retransmission of packets and acknowledgement packets.

\subparagraph*{Conclusion}

UDP and TCP are general purpose communication protocols which only offer low level operations such as setting up communications and transferring unstructured data. They are implemented efficiently and offer the building blocks for more complex interactions which have to be implemented by the software.

\subsubsection{Request-reply protocols}

Request-reply protocols refer to a pattern imposed on an underlying messaging system to allow communication between a client/server setting. A client will send a request to a client application using a |doOperation| method, which will then wait for a reply and return the answer when it is received. The server can call a |getRequest| method to (wait for and) receive a message and then use a |sendReply| method to send a reply back to the client after the request has been executed. In this system, acknowledgement messages are redundant as replies are used instead, a connection structure is not nessecary and there is no need for flow control.

Because of the way request-reply protocols work, the reliability that comes with the TCP protocol causes a lot of overhead, as acknowledgements are redundant, the size of data packets is limited and communication occurs sporadically. This means that the TCP connection concept and streams or flow control are not needed. UDP can be used instead to reduce the overhead and improve efficiency, but UDP does have issues with reliability and may require requests or replies to be resent.

\subsubsubsection{Case study: HTTP}

The original HTTP protocol had the server and clients communicate with a simple sequence of events. First a connection would be opened, the client would send a request and the server would reply. After that had occurred, the connection was closed. With the introduction of the HTTP 1.1 protocol, persistent connections using TCP were added to handle multiple requests from the same server.

\subsubsection{Asynchronous communication}

Request-reply protocols are coupled with time. Clients are constantly waiting for replies. They do this when establishing connections, when they transfer data and when they are closing the connections. As a result, the server they connect with must serve replies correctly and at a fast enough rate, to prevent clients from hanging during their wait cycle because the reply does not arrive or does not arrive fast enough. This coupling with time can be addressed at the client or server side by implementing things such as buffers and time-outs. It can however also be handled at the protocol level by using message queues.

A \textbf{message queue} is usually implemented on the server side. One or more clients connected to the server will send it messages. The server will then sequentially handle these. The server can employ one or more worker processes to handle the messages by removing them from the queue when they are available. Multiple servers can also be deployed (for decoupling in space), but this requires the addition of a message broker and leads to a \textbf{publish \& subscribe messaging system}. Here clients can publish their (typed) messages to the broker. Servers can register with the broker to subscribe to a type of client (messages) they can handle. The broker will then distribute messages from the clients to the appropriate servers.

\subsubsection{Remote procedure calls}

In a traditional application, there is a main program which calls procedures. In a distributed system, the same effect can be achieved by grouping the procedures into servers and having the main programs act as clients. When a client invokes a procedure on a server remotely, it will act like a conventional procedure call. This is achieved by \textbf{RPC middleware} though the use of \textbf{stubs}. These stubs are interfaces which represent procedures on the server as they would appear in a local environment, allowing the client to call them and essentially be unaware of how they are implemented, which in this case would be using a messaging system to call them remotely.

To understand remote procedure calls, it is important to note that there is a stub at the client side and a stub at the server side. The client stub allows the client to call procedures transparently, where as the stub on the server side communicates client requests back and forth between the actual implementation on the server. An RPC invocation sequence may look as follows:

\begin{enumerate}
\item The application calls a procedure on the client stub.
\item The client stubs marshalls the parameters of the calls and passes it to the module used to communicate with the server.
\item The communication module in the clients transmits the message to the server.
\item The communication module in the server receives the message and relays it to the dispatcher service.
\item The dispatcher service sends the message to the appropriate server stub.
\item The server stub unmarshalls the parameters for the procedure call and invokes the correct procedure on the server.
\item The procedure on the server returns its result to the server stub.
\item The server stub marshalls the answer and passes it to the communication module.
\item The communication module in the server transmits the message to the client.
\item The communication module in the client relays the message to the client stub.
\item The client stub returns the result through the method interface called by the application.
\end{enumerate}

In this system, the code of both the client and the server are completely independent from the actual communication system. Remote procedure invocation is transparent to the client, makign it unaware that the method is executed remotely. An \textbf{Interface Definition Language (IDL)} is used as a language independent of those used for the client and server to generate the stubs, allowing for clients and servers written in different languages to work compatibly.

\subsubsubsection{Design issues}

\begin{itemize}
\item RPC can be implemented in different ways, generally two classes stand out. There's a set of RPC systems which are integrated with a particular programming language. This includes Argus (a programming language which extends the CLU language) and Arjuna (a C++ library). Another set is those with RPC based on a special IDL. This includes Sun RPC, ANSA RPC and OSF/DCE.
\item An IDL describes operation signatures. Interface compiler must be provided to generate client and server stubs in various languages. This allows for an abstraction of heterogeneity accross various platforms.
\item When handling exceptions in an RCP, failures can not be hidden. A failure can be caused due to network or server failure, but the client can not distinguish between the two. Failures can be supported with language specific implementations, using return codes of functions to signal that a failure has occurred or by sing an extension provided by the IDL.
\item There's four ways of handling RPC delivery guarantees.
\begin{enumerate}
\item \textbf{Maybe}: An RPC call may or may not be executed by the server. This happens when there's no fault handling.
\item \textbf{At-least-once}: An RPC call returns either the result or an error that no result was received. This can be achieved by re-executing the request if it is lost in transit.
\item \textbf{At-most-once}: An RPC call returns either the result, ensuring the call executed just once, or an error that indicates there was no result and either it execute once or not at all. This can be achieved by retransmitting the reply from the server and filtering duplicate requests.
\item \textbf{Exactly-once}: An RPC call returns the correct result, ensuring that it was executed at least once and only once. This is very difficult, if not impossible to achieve as failures are always possible.
\end{enumerate}
\item Transparency is an important concept in RPC; one usually aims to make remote calls as similar to local calls as possible. RPC is however more vulnerable to failure, which should not be hidden. Some other difficulties include that calling instructions has to happen very differently and that there is no shared memory between the client and server. There has to be a balance between programming convenience and true transparency.
\item The interface compiler is responsible for generating the client stub procedure, server stub procedure, generating code for marshalling and unmarshalling of the arguments for each type and generating the header for the server procedure. A binder interface is provided to link a client to a server at execution time. A server can register itself to receive RPC calls with the binder using a |register| method and a client can look up a service using a |lookup| mehod. The binder itself is either located at a well known host address, the OS could be responsible for the bindings or the client could broadcast a message to contact a binder on the network.
\item \textbf{Asynchronous RPC} can be used as an alternative to synchronous RPC when the system has to handle many requests for amsll amounts of information that requires limited processing or when parallel requests have to be sent to several servers. this is useful because servers waste a lot of time idling in between RPC calls to wait for a new call. This could further be optimized by buffering requests at the client until a server is available to handle it, or by having a client resume execution if no reply is needed for an RPC call.
\begin{itemize}
\item \textbf{Call streams} can be implemented as a connection oriented RPC system that mixes synchronous and asynchronous calls. A stream allows the system to preserve the order of the calls.
\item \textbf{Promises} allow clients to resume other executions and retrieve the results of RPC calls later. A promise is created at the time of the call and stored the results of the call once received. A client can invoke methods on a promise object, to |get| a result, |await| a result or check if a result is available. Commonly used altenative names for promises are \textbf{futures}, \textbf{tickets} and \textbf{continuations}.
\end{itemize}
\end{itemize}

\subsubsubsection{Conclusion}

RPC is a familiar paradigm as it closely resembles regular procedure calls. It is one fot he fundamendal basics of distributed programming. It does have limitations, as failures have to be handled by clients and these failures are unpredictable and untraceable. It has no support for transactions and it only uses one-to-one communcation.

\newpage

\section{Practical Information}

\subsection{Java RMI}

An application using Java RMI consists out of three essential components.

\begin{enumerate}
\item The \textbf{rmiregistry}, which is the name server that keeps track of all registered server objects and their name. This can also be part of the server application, but is often ran as a separate service.
\item The \textbf{server application}, which implements the back-end logic of the system.
\item The \textbf{client application}, which implements the front-end logic of the system and uses RMI to communicate with the server.
\end{enumerate}

All RMI specific classes can be found in the |java.rmi| package.

\subsubsection{Server Basics}

To set up a server with Java RMI, you have to provide provide an interface for every object the client has to be able to use remotely. These interfaces must...

\begin{itemize}
\item ...extend the |Remote| interface.
\item ...implement each method the client needs access to.
\item ...have each method throw |RemoteException|.
\item ...only use datatypes which can be serialized or other interfaces which subclass |Remote|.
\end{itemize}

The classes which implement the remote interfaces must...

\begin{itemize}
\item ...extend the |UnicastRemoteObject| class.
\item ...call |super()| in their constructor.
\item ...have their constructor throw |RemoteException|.
\end{itemize}

The server must connect to the rmiregistry name service and register any object which the client must be able to \textbf{access by name}! Not every remote object must be registered! Objects which can be accessed by reference, when they are passed through methods, do not have to be registered.

The registry is accessed by URI, defined as {\small\ttfamily rmi://[host]:[port]/[name]}, where |name| is the name of the object you are registering. Registration is achieved by calling |Naming.bind(uri, object)|.

\subsubsection{Client Basics}

The client application must connect to the rmiregistry service to retrieve interfaces to named object instances. The connection is established by calling |LocateRegistry.getRegistry(host, port)|, which returns a |Registry| instance. An interface to a remote object is retrieved by calling |lookup(name)| on the |Registry| object returned by the |LocateRegistry.getRegistry()| method. This object can be cast to the |Remote| interface that was implemented for the specific object.

\subsubsection{rmiregistry}

The rmiregistry service has to be started before the server and client are. It must be started in the root of the classpath of the application, or it has to be provided a location to look for the |Remote| interface classes using the {\small\ttfamily -J-Djava.rmi.server.codebase} parameter, with the path syntax {\small\ttfamily file://[path]} for a jar file or a folder with class files. (A folder name has to end with a path delimiter). A port number is specified as an argument as well. Running in the classpath root can for example be achieved by simply calling {\small\ttfamily rmiregistry 5000} to run on port 5000.

\subsubsection{Example}

A simple hello world example with a client and server. The rmiregistry service is running on port 5000.

\begin{lstlisting}
public interface RemoteHello extends Remote {

    String getHello() throws RemoteException;

}
\end{lstlisting}

\begin{lstlisting}
public class HelloServer extends UnicastRemoteObject implements RemoteHello {

    private String hello;

    public HelloServer(String hello) throws RemoteException {
        super();
        this.hello = hello;
    }

    public static void main(String[] args) throws RemoteException, AlreadyBoundException, MalformedURLException {
        HelloServer server = new HelloServer("Hello World!");
        Naming.bind("rmi://localhost:5000/hello", server);
    }

    @Override
    public String getHello() throws RemoteException {
        return hello;
    }

}
\end{lstlisting}

\begin{lstlisting}
public class HelloClient {

    private Registry registry;
    private RemoteHello hello;

    public HelloClient() throws RemoteException, NotBoundException {
        registry = LocateRegistry.getRegistry("localhost", 5000);
        hello = (RemoteHello) registry.lookup("hello");
    }

    public void print() throws RemoteException {
        System.out.println(hello.getHello());
    }

    public static void main(String[] args) throws RemoteException, NotBoundException {
        new HelloClient().print();
    }

}
\end{lstlisting}

\subsubsection{Exceptions}

The RMI system implements exceptions using the |RemoteException| class. When the server throws an exception, this can be forwarded to the client directly by catching it on the server and throwing a new exception as |new RemoteException("message", originalException)|. The original exception can also be hidden from the client by just passing a message string to the |RemoteException| constructor.

\subsubsection{Notes}

What follows is a list of important notes, pitfalls and FAQs.

\begin{itemize}
\item Collections such as |List| and |Set| can be serialized and as such, can be used by |Remote| interfaces as arguments or return values for methods.
\item The |Naming.bind()| method will throw an exception when the name is already in use. |Naming.rebind| can create or overwrite an existing binding.
\item When using an external rmiregistry service, the service will retain bindings from a previous run of your server, so it must be restarted when the server is restarted. Note that external does not necessarily refer to a service running on an external host, but the rmiregistry running as an individual service, not being part of the server application, which can be done using the |LocateRegistry.createRegistry()| method.
\item As mentioned previously, only objects which must be accessed by name have to be registered. It can be interesting to implement a single API object which can then return other remote objects by reference.
\item When the server receives an object reference from the client as a |Remote| interface, it can not be cast back to the original object. The server must either use the interface as well, or keep a mapping of every object and its reference.
\item When running your server on an external host, you must specify the hostname (or IP) where the server can be found as an argument when running the server. The server will forward this hostname to the rmiserver, allowing the client to correctly connect to the server for remote object access. The argument is {\small\ttfamily -Djava.rmi.server.hostname=[hostname]}.
\end{itemize}

\end{document}

